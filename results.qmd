---
title: "Data Analytic Strategy and Results"
author: "Kendra Wyant and Jeongyeol Kwon"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| message: false
#| echo: false

library(tidyverse)

aucs <- read_csv(here::here("data/auc_3_x_10.csv"),
                 show_col_types = FALSE)
posteriors <- read_csv(here::here("data/posteriors.csv"),
                       show_col_types = FALSE)
pp_tibble <- read_csv(here::here("data/pp_perf_tibble.csv"),
                       show_col_types = FALSE)
contrasts <- read_csv(here::here("data/contrast.csv"),
                      show_col_types = FALSE)
```

## Model Training and Evaluation

### Model Configurations

*Jeongyeol: I would add a paragraph here stating the type of neural network model you trained. I would also mention the different candidate model configurations you considered (e.g., optimizer, hyperparameters, penalties etc.)*

*Then I would put something here to describe the final five models - NN full, NN LV1, NN LV2... (you don't need to include XGBoost). I am not sure what LV stands for so I would briefly define it. Maybe also think about the best way to reference these different models in the paper.* 


### Cross-validation

*Just double check the paragraph below is accurate.*

We used 3 repeats of 10-fold cross-validation to evaluate our models with auROC. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). We used grouped cross-validation to assign all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant’s data from their own data. 

### Bayesian Model

We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) from the 30 held-out test sets for our five best models. We used the default weakly informative priors. We set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within repeat.  

From the Bayesian model we obtained the posterior distribution (transformed back from logit) and Bayeisan CIs for auROCs all five models. To evaluate our models’ overall performance we report the median posterior probability for auROC and Bayesian CIs. This represents our best estimate for the magnitude of the auROC parameter for each model. 

We then conducted Bayesian model comparisons to determine the probability that the penalized models’ performances differed systematically from the full model. We also report the precise posterior probability for the difference in auROCs and the 95% Bayesian CIs for these comparisons.

## Results

*For now I am just keeping this short and to the point since we don't know what the final results are yet that we are going to write up. Feel free to edit or add anything.*


### Sampling Frequency

*Kendra will add results here after getting the data.*

### Model Evaluation

Median auROCs for the XGBoost and 5 neural network models are presented in the table below.
```{r}
#| echo: false

aucs |> 
  group_by(n_repeat) |> 
  summarise(across(xgboost:nn_lv4, ~median(.x))) |> 
  ungroup() |> 
  summarise(across(xgboost:nn_lv4, ~median(.x))) |> 
  glimpse()
```

Median posterior probabilities and 95% Bayesian credible intervals for auROC for the XGBoost and 5 neural network models are presented in the table below.
```{r}
#| echo: false

pp_tibble |> 
  kableExtra::kbl(digits = 3)
```

Histograms of posterior probabilities for auROC for the 5 neural network models are presented below.  
```{r}
#| echo: false

posteriors |> 
  filter(model != "xgboost") |> 
  mutate(model = factor(model, 
                        levels = c("nn_full", "nn_lv1", "nn_lv2", 
                                   "nn_lv3", "nn_lv4"))) |>
  ggplot() + 
  geom_histogram(aes(x = posterior), fill = "light grey", color = "black", 
                 bins = 30) +
  geom_segment(mapping = aes(y = 3000, yend = 3600, x = pp_median, 
                             xend = pp_median),
               data = subset(pp_tibble, model != "xgboost")) +
  geom_segment(mapping = aes(y = 3400, yend = 3400, x = pp_lower, xend = pp_upper),
                data = subset(pp_tibble, model != "xgboost")) +
  facet_wrap(~model, ncol = 1) +
  ylab("Posterior Probability Frequency") +
  xlab("Area Under ROC Curve") +
  theme_classic() 
```


### Model Comparisons

*Kendra will add a description of the model comparisons below once I see how we are referring to the different models. Essentially these comparisons suggest that we can go down to using about 40% of surveys (LV1) without any difference in model performance. It appears that the model does perform differently when going down to 25% of surveys (LV2) but this is not likely a clinically meaningful drop in performance. LV3 performs worse than LV1, but again it is not likely a clinically meaningful drop.*

```{r}
#| echo: false

contrasts |> 
  kableExtra::kbl(digits = 3)
```

