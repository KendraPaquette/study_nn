{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596662bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6edc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from munch import Munch\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e3ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ema.csv', sep=',')\n",
    "df = df.sort_values(['subid', 'dttm_obs'])[['dttm_obs', 'subid', 'ema_1', 'ema_2', 'ema_3', 'ema_4', \\\n",
    "                                            'ema_5', 'ema_6', 'ema_7', 'ema_8', 'ema_9', 'ema_10']]\n",
    "\n",
    "# Handling NA: drop the entire row if 4th entry is NA \n",
    "# For morning-only survey, set dummy=1 if survey reported, otherwise dummy=0\n",
    "df = df.fillna(-6.0)\n",
    "df = df[df['ema_4'] > -1]\n",
    "df['ema_dummy'] = (df['ema_8'] > -1)\n",
    "df.loc[df['ema_8'] < 0, ['ema_8', 'ema_9', 'ema_10']] = 0\n",
    "\n",
    "# Encoding: 'Yes/No -> 0/1'\n",
    "df = df.replace('No',0)\n",
    "df = df.replace('Yes',1)\n",
    "\n",
    "# Date-time encoding -- in hour unit \n",
    "df['date'] = (pd.to_datetime(df['dttm_obs']).astype('int64') // (10**9)) / 3600\n",
    "df2 = df[['subid', 'ema_1', 'ema_2', 'ema_3', 'ema_4', \\\n",
    "            'ema_5', 'ema_6', 'ema_7', 'ema_8', 'ema_9', 'ema_10', 'ema_dummy', 'date']]\n",
    "\n",
    "\n",
    "time_df = pd.read_csv('labels_1day.csv', sep=',')\n",
    "time_df = time_df.sort_values(['subid', 'dttm_label']) \n",
    "time_df['date'] = (pd.to_datetime(time_df['dttm_label']).astype('int64') // (10**9)) / 3600\n",
    "time_df = time_df.replace('no',0)\n",
    "time_df = time_df.replace('yes',1)\n",
    "\n",
    "time_df = time_df[['subid', 'lapse', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2e7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import calculate_feature\n",
    "\n",
    "def input_data(org_data, time_label):\n",
    "    j = [0,0,0,0,0]\n",
    "    fj = 0\n",
    "    window = [12, 24, 48, 72, 168]\n",
    "    \n",
    "    dataset = []\n",
    "    tq, ts = 0, 1\n",
    "    \n",
    "    # offset starting date-time to 0 \n",
    "    offset_time = org_data[0,-1] + 0.0\n",
    "    org_data[:,-1] -= offset_time\n",
    "    time_label[:,-1] -= offset_time\n",
    "    \n",
    "    while( tq < time_label.shape[0] and ts < org_data.shape[0] ):\n",
    "        time = time_label[tq,-1]        \n",
    "        for k in range(5):\n",
    "            while( j[k] < ts-1 and org_data[j[k], -1] < time - window[k] ):\n",
    "                j[k] = j[k] + 1\n",
    "\n",
    "        X2 = calculate_feature(org_data[:ts, 1:-1], org_data[ts-1, 1:-1], j)\n",
    "        if(ts == org_data.shape[0] or (tq < time_label.shape[0] and time_label[tq, -1] < org_data[ts, -1])):\n",
    "            y = 1 if( time_label[tq,1] > 0 ) else 0\n",
    "            dataset.append({'type':'query', 'out': y, 'time': time_label[tq,-1], 'obs': org_data[ts,1:-1]+0.0, 'fullX': X2 })\n",
    "            while(tq < time_label.shape[0] and time_label[tq,-1] < org_data[ts,-1] ):\n",
    "                tq += 1\n",
    "        else:\n",
    "            y = 1 if( tq < time_label.shape[0] and time_label[tq,1] > 0 ) else 0\n",
    "            dataset.append({'type':'survey', 'obs': org_data[ts, 1:-1]+0.0, 'time': org_data[ts,-1], 'out':y, 'fullX': X2})\n",
    "            ts += 1\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8442e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = df2.values.astype(np.float32)\n",
    "time_raw_data = time_df.values.astype(np.float32)\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "env_list, subid_list = [], []\n",
    "print_list = []\n",
    "for subid in range(270):\n",
    "    # print(subid)\n",
    "    sub_data = raw_data[raw_data[:,0]==(subid+1)]\n",
    "    time_label = time_raw_data[time_raw_data[:,0]==(subid+1)]\n",
    "    if( len(sub_data) == 0 or len(time_label) == 0 ):\n",
    "        continue\n",
    "          \n",
    "    subid_list.append(subid)\n",
    "    print_list.append(subid+1)\n",
    "    env_list.append(input_data(sub_data, time_label))\n",
    "\n",
    "print(print_list, time.time() - st, env_list[0][0]['fullX'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70742439",
   "metadata": {},
   "source": [
    "# XGBoost Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def input_xgboost_data(org_data, time_label):\n",
    "    j = [0,0,0,0,0]\n",
    "    fj = 0\n",
    "    window = [12, 24, 48, 72, 168]\n",
    "    \n",
    "    dataset = []\n",
    "    tq = ts = 0\n",
    "\n",
    "    while( ts < org_data.shape[0] ):\n",
    "        while(tq < time_label.shape[0] and time_label[tq,-1] < org_data[ts,-1] ):\n",
    "            tq += 1\n",
    "            \n",
    "        if( tq >= time_label.shape[0] ):\n",
    "            break\n",
    "            \n",
    "        time = time_label[tq,-1]\n",
    "        while( ts < org_data.shape[0] and org_data[ts,-1] <= time ):\n",
    "            ts += 1\n",
    "        \n",
    "        for k in range(5):\n",
    "            while( j[k] < ts-1 and org_data[j[k], -1] < time - window[k] ):\n",
    "                j[k] = j[k] + 1\n",
    "                              \n",
    "        X = calculate_feature(org_data[:ts, 1:-1], org_data[ts-1, 1:-1], j)\n",
    "        y = 1 if ( time_label[tq,1] > 0 ) else 0        \n",
    "        dataset.append({'in':X, 'out':y})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def xgboost_main(train_index, test_index):\n",
    "    trainset = []\n",
    "    for m in train_index:\n",
    "        subid = subid_list[m] + 1\n",
    "        sub_data = raw_data[raw_data[:,0] == subid] \n",
    "        time_label = time_raw_data[time_raw_data[:,0]==subid]                \n",
    "        trainset.extend(input_xgboost_data(sub_data, time_label))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=500000, shuffle=True, drop_last=False)\n",
    "\n",
    "    testset = []\n",
    "    for m in test_index:\n",
    "        subid = subid_list[m] + 1\n",
    "        sub_data = raw_data[raw_data[:,0] == subid]     \n",
    "        time_label = time_raw_data[time_raw_data[:,0]==subid]                \n",
    "        testset.extend(input_xgboost_data(sub_data, time_label))\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=60000, shuffle=False, drop_last=False)\n",
    "\n",
    "    \n",
    "    # XG Boost Train\n",
    "    data_iter = iter(train_loader)\n",
    "    train_data = next(data_iter)\n",
    "    X = train_data['in']\n",
    "    y = train_data['out']\n",
    "\n",
    "    test_iter = iter(test_loader)\n",
    "    test_data = next(test_iter)\n",
    "\n",
    "    # Create regression matrices\n",
    "    dtrain_reg = xgb.DMatrix(X, y)\n",
    "\n",
    "    X_test = test_data['in']\n",
    "    y_test = test_data['out']\n",
    "    dtest_reg = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "    params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\", \"device\": \"cuda\", \"max_depth\": 5}\n",
    "\n",
    "    print(X.shape, X_test.shape)\n",
    "    \n",
    "    n = 10\n",
    "    xgmodel = xgb.train(\n",
    "       params=params,\n",
    "       dtrain=dtrain_reg,\n",
    "       num_boost_round=n,\n",
    "    )\n",
    "\n",
    "    preds = xgmodel.predict(dtest_reg)\n",
    "    print(len(y_test[y_test>0]), len(preds[preds>0.1]), len(preds))\n",
    "    xgauc = roc_auc_score(y_test, preds)\n",
    "    print('XgBoost: AUC = %.2f' % xgauc)  #1\n",
    "         \n",
    "    return xgauc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e06ee",
   "metadata": {},
   "source": [
    "# Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env, VecEnv\n",
    "\n",
    "# parallel env runs\n",
    "num_process = 31\n",
    "\n",
    "# action encoding: \n",
    "# 0 - no survey or predict\n",
    "# 1 - yes survey or predict\n",
    "action_dim = 1\n",
    "action_max = 2\n",
    "\n",
    "# feature encoding: mean of survey items\n",
    "feature_dim = 7\n",
    "\n",
    "# y encoding\n",
    "# 0 - no lapse within 24hr\n",
    "# 1 - yes lapse within 24hr\n",
    "state_dim, embed_dim, y_max = 143, 128, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e414a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e8580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get trajectory sample batch\n",
    "def get_sample(traj, model, device):\n",
    "    action = traj['action'].reshape(-1, 1).to(device)  # [B, n]\n",
    "    reward = traj['reward'].reshape(-1, 1).to(device)  # [B, n]    \n",
    "    s1 = model(traj['state'].to(device)).detach()\n",
    "    s2 = model(traj['nstate'].to(device)).detach()\n",
    "    t1 = traj['type'].to(int).to(device)\n",
    "    t2 = traj['ntype'].to(int).to(device)\n",
    "    done = traj['done'].reshape(-1, 1).to(device)\n",
    "\n",
    "    return t1, s1, action, reward, t2, s2, done\n",
    "\n",
    "# train function (without extra features)\n",
    "def train(venv, model, agent, feature_optimizer, predict_buffer, max_iter = 50, freq=1, burn_in = -1, \n",
    "          ca = 0.025, cb = 0.2, full_random = False):\n",
    "    ce_criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    mse_criterion = torch.nn.MSELoss(reduction='none')\n",
    "    LReLU = torch.nn.LeakyReLU(0.1)\n",
    "    ReLU = torch.nn.ReLU()\n",
    "    buffer = ReplayBuffer()\n",
    "    n_process = 31\n",
    "    cumr = []\n",
    "    \n",
    "    for episode in range(max_iter):\n",
    "        if( (episode + 1) % 20 == 0 and episode > burn_in ):\n",
    "            #mid_full_auc = test(venv, model, agent, 'full', freq=freq)\n",
    "            mid_auc = test(venv, model, agent)            \n",
    "            #print(mid_full_auc, mid_auc)            \n",
    "            cumr = cumr[len(cumr)//2:]\n",
    "            \n",
    "        X, info = venv.reset(mode='train')\n",
    "        # print(n_process, num_process)\n",
    "\n",
    "        # initialize buffers / debug variables\n",
    "        sample_traj = []\n",
    "        predict_samples = []\n",
    "        \n",
    "        measured_ta = []\n",
    "        measured_tr = []\n",
    "        types = torch.zeros(n_process).to(int)\n",
    "        \n",
    "        # generate trajectories\n",
    "        for t in range(1000):\n",
    "            PX = X\n",
    "            embed = model(PX.to(device))\n",
    "            p = random.random()\n",
    "            \n",
    "            # sample at least once a week\n",
    "            if( t < 10 or p < 0.05 ):\n",
    "                actions = torch.zeros(n_process, action_dim).to(int) + 1\n",
    "            elif( episode < burn_in ):\n",
    "                actions = torch.zeros(n_process, action_dim).to(int) + (p < 1. / freq) \n",
    "            else:\n",
    "                actions, _, _ = agent.select_action(embed, types.to(device))\n",
    "                actions = actions.detach().cpu()\n",
    "            \n",
    "            X, X2, rewards, dones, infos = venv.step(actions)\n",
    "            all_done = True\n",
    "            \n",
    "            predict_ys, predict_ft = model.predict_forward(embed)\n",
    "            predict_ft = predict_ft.detach().cpu()\n",
    "            predict_ys = nn.Softmax(dim=-1)(predict_ys).reshape(-1, y_max).detach().cpu()\n",
    "            true_ys = nn.Softmax(dim=-1)(model.predict_forward(model(X2.to(device)))[0]).reshape(-1,y_max).detach().cpu()\n",
    "            \n",
    "            for i in range(n_process):\n",
    "                if( infos[i] == -1 ):\n",
    "                    continue\n",
    "                \n",
    "                all_done = False\n",
    "                if (infos[i]['type'] == 'survey'):\n",
    "                    # if too much prediction error, then...\n",
    "                    var_sample = 0.1 * (predict_ft[i][0]-infos[i]['feature'][0])**2 \\\n",
    "                                        + 0.01 * ((predict_ft[i][1:]-infos[i]['feature'][1:])**2).mean()\n",
    "\n",
    "                    if( actions[i] == 1 ):\n",
    "                        # measure encouraing rewards based on variances (larger variance, more desired to measure)\n",
    "                        rewards[i] += var_sample\n",
    "                        \n",
    "                    if( i == 11 and (t+1)%50 == 0 ):\n",
    "                        print( var_sample * 10, actions[i], rewards[i] )\n",
    "                    \n",
    "                    # predict if it is something you would've have seen if you measured\n",
    "                    predict_samples.append({'type':'survey', 'in': PX[i], 'out': infos[i]['lapse'], 'feature': infos[i]['feature']})    \n",
    "                \n",
    "                else:\n",
    "                    y, cy = infos[i]['lapse'], predict_ys[i][1] \n",
    "                    # rewards[i] -= 10 * (y * LReLU(5*(dy - cy))**2 + (1 - y) * LReLU(5*(cy - dy))**2)\n",
    "                    # rewards[i] -= 50 * (cy - dy)**2\n",
    "                    rewards[i] += 2 * ( y * np.log(cy + 1e-4) + (1-y) * np.log((1-cy) + 1e-4) )\n",
    "\n",
    "                    # rewards[i] -= 10 * (y * ReLU(5*(cb - cy))**2 + (1 - y) * min(ReLU(5*(cy - ca))**2, 0.2))\n",
    "                    ## rewards[i] -= y * 25 * LReLU(cb - cy) + (1 - y) * 5 * LReLU(cy - ca)                    \n",
    "                    predict_samples.append({'type':'query', 'in': PX[i], 'out': infos[i]['lapse'], 'feature': infos[i]['feature']})\n",
    "                    \n",
    "                ptype = types[i]\n",
    "                types[i] = 1 if( infos[i]['next_type'] == 'query' ) else 0\n",
    "                # wait until prediction network becomes reliable\n",
    "                if( episode > burn_in ):\n",
    "                    sample_traj.append({'type': ptype, 'state': PX[i], 'action': actions[i], 'reward': rewards[i], \\\n",
    "                                'ntype': types[i], 'nstate': X[i], 'done': dones[i]})\n",
    "            \n",
    "            if( episode > burn_in ):\n",
    "                cumr.extend(rewards)\n",
    "            \n",
    "            if( all_done ): \n",
    "                break\n",
    "                \n",
    "        predict_buffer.add_episodes(predict_samples)\n",
    "        buffer.add_episodes(sample_traj)\n",
    "        \n",
    "        if( episode < burn_in ):\n",
    "            continue\n",
    "            \n",
    "        #######################################################\n",
    "        # Training Phase        \n",
    "        #######################################################\n",
    "        # 1. train prediction first\n",
    "        feat_train_loaders = predict_buffer.get_loader()\n",
    "        feat_data_iter = iter(feat_train_loaders)\n",
    "        train_iters = 2000 if (episode == burn_in) else 20\n",
    "        for j in range(0, train_iters):\n",
    "            try:\n",
    "                traj = next(feat_data_iter)\n",
    "\n",
    "                refs = traj['in'].to(device)\n",
    "                embeds = model(refs)\n",
    "                out_ys, out_features = model.predict_forward(embeds)\n",
    "                ys = traj['out'].reshape(-1).to(device)\n",
    "                features = traj['feature'].reshape(-1, feature_dim).to(device)\n",
    "                tps = np.array(traj['type'])\n",
    "                wf = (tps=='survey')\n",
    "                wy = (tps=='query')\n",
    "                \n",
    "                predict_loss = ce_criterion(out_ys[wy], ys[wy]).mean()\n",
    "                predict_loss += 0.1 * mse_criterion(out_features[wf], features[wf]).mean()\n",
    "                predict_loss.backward()\n",
    "                \n",
    "                feature_optimizer.step()\n",
    "                feature_optimizer.zero_grad()\n",
    "\n",
    "            except StopIteration:\n",
    "                feat_train_loaders = predict_buffer.get_loader()\n",
    "                feat_data_iter = iter(feat_train_loaders)\n",
    "                continue\n",
    "        \n",
    "        if( episode < burn_in + 3 ):\n",
    "            continue\n",
    "        print('episode: ', episode, 'loss: ', predict_loss, 'average reward: ', np.mean(cumr))\n",
    "            \n",
    "        # 2. train SAC\n",
    "        train_loaders = buffer.get_loader(batch_size=4096)\n",
    "        data_iter = iter(train_loaders)\n",
    "\n",
    "        for j in range(0, 500):\n",
    "            try:\n",
    "                traj = next(data_iter)\n",
    "                batch = get_sample(traj, model, device)\n",
    "\n",
    "            except StopIteration:\n",
    "                train_loaders = buffer.get_loader(batch_size=4096)\n",
    "                data_iter = iter(train_loaders)\n",
    "                continue\n",
    "\n",
    "            agent.update(batch)\n",
    "\n",
    "        agent.update_target()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ca7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9447ee15",
   "metadata": {},
   "source": [
    "# Test Reinforcement Learning Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d51915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(venv, model, agent, mode='policy', freq=1, run_id=0, kfold_id=0, penalty=-1., verbose=True):\n",
    "    scatter = []\n",
    "    num_process = len(venv.test_ids)\n",
    "    \n",
    "    cnt, n_measure, n_hit = [0 for i in range(num_process)], [0 for i in range(num_process)], [0 for i in range(num_process)]\n",
    "    cnt2 = [0 for i in range(num_process)]\n",
    "    cnt3 = [0 for i in range(num_process)]\n",
    "\n",
    "    y, outy = [], []\n",
    "\n",
    "    X, info = venv.reset(mode='test')\n",
    "    types = torch.zeros(num_process).to(int)\n",
    "    for t in range(1000):\n",
    "        PX = X\n",
    "        embeds = model(PX.to(device))\n",
    "        p = random.random()\n",
    "        if( t < 10 or p < 0.02 ):\n",
    "            actions = torch.zeros(num_process, action_dim).to(int) + 1\n",
    "        elif( mode == 'full' ):\n",
    "            actions = torch.zeros(num_process, action_dim).to(int) + (p < 1./freq)\n",
    "        else:\n",
    "            actions, _, _ = agent.select_action(embeds, types.to(device), explore=False)\n",
    "            \n",
    "        X, _,_,_, infos = venv.step(actions)\n",
    "        predict_ys, _ = model.predict_forward(embeds)\n",
    "        predict_ys = predict_ys.detach().cpu()\n",
    "   \n",
    "        # add transition data to dataset\n",
    "        all_done = True\n",
    "        for i in range(num_process):\n",
    "            if( infos[i] == -1 ):\n",
    "                continue\n",
    "\n",
    "            all_done = False\n",
    "            if (infos[i]['type'] == 'query'):\n",
    "                check_y = nn.Softmax(dim=-1)(predict_ys[i])                \n",
    "                y.append(infos[i]['lapse'])\n",
    "                outy.append(check_y[1])\n",
    "                \n",
    "                if( infos[i]['lapse'] > 0 ):\n",
    "                    n_hit[i] += 1 \n",
    "                    if(check_y[1] < 0.05):\n",
    "                        cnt2[i] += 1\n",
    "                    if(check_y[1] < 0.09):\n",
    "                        cnt3[i] += 1                        \n",
    "            else:\n",
    "                cnt[i] += 1\n",
    "                n_measure[i] += 1 if( actions[i] % 2 == 1 ) else 0\n",
    "                                    \n",
    "            types[i] = 1 if( infos[i]['next_type'] == 'query' ) else 0\n",
    "\n",
    "        if( all_done ): \n",
    "            break\n",
    "\n",
    "    for i in range(num_process):\n",
    "        scatter.append([n_measure[i]/cnt[i], n_hit[i]/cnt[i]])\n",
    "        if( verbose ):\n",
    "            print(venv.test_ids[i], ':', n_measure[i], n_hit[i], cnt[i], cnt2[i], cnt3[i])\n",
    "    \n",
    "    \n",
    "#     path = os.path.join(\"./results\", f\"run_{run_id}_kfold_{kfold_id}_penalty_{penalty}.pt\")\n",
    "#     things_to_save = {\n",
    "#         'test_ids': venv.test_ids,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'agent_state_dict': agent.save_dict()\n",
    "#     }\n",
    "#     torch.save(things_to_save, path)\n",
    "    \n",
    "    scatter = np.array(scatter)\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     plt.figure(figsize=(4, 3))  \n",
    "\n",
    "#     plt.scatter(scatter[:,1], scatter[:,0])\n",
    "#     plt.xlabel(\"Average Lapse Count\")\n",
    "#     plt.ylabel(\"Survey Ratio\")\n",
    "#     plt.show()\n",
    "\n",
    "    outy0 = np.array(outy)[np.array(y)==0]\n",
    "    outy1 = np.array(outy)[np.array(y)==1]   \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))     \n",
    "    \n",
    "    axes[0].hist(outy0, bins=50, alpha=0.5, label='Class 0')\n",
    "    axes[1].hist(outy1, bins=50, alpha=0.5, label='Class 1')\n",
    "#     axes[0].xlabel('Score P(y=1)')\n",
    "#     axes[1].xlabel('Score P(y=1)')\n",
    "#     axes[0].ylabel('Frequency')\n",
    "#     axes[1].ylabel('Frequency')\n",
    "    \n",
    "    axes[2].scatter(scatter[:,1], scatter[:,0])\n",
    "#     axes[2].xlabel(\"Average Lapse Count\")\n",
    "#     axes[2].ylabel(\"Survey Ratio\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print('Average Survey Ratio:', np.mean(scatter[:,0]))\n",
    "\n",
    "    import xgboost as xgb\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    fgauc = roc_auc_score(y, outy)\n",
    "    print('AUC = %.2f' % fgauc)  #1\n",
    "    return fgauc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546951cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from SACD import init_SACD_agent\n",
    "from model import build_model\n",
    "from buffer import ReplayBuffer\n",
    "  \n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# weight-decay params: {1e-3, 5e-4, 1e-4, 5e-5, 1e-5}\n",
    "\n",
    "def k_fold_test(k=5, run_id=0):\n",
    "    shuffled_index = list(range(151))\n",
    "    random.shuffle(shuffled_index)\n",
    "\n",
    "    AUC_dict={}\n",
    "    AUC_dict['xg'] = []\n",
    "    for freq in [1,5,10]:\n",
    "        AUC_dict[freq] = []\n",
    "        \n",
    "    for i in range(k):\n",
    "        st,et = i*151//k, (i+1)*151//k\n",
    "\n",
    "        test_index = shuffled_index[st:et]\n",
    "        train_index = list(set(range(151)) - set(test_index))\n",
    "        xgauc = xgboost_main(train_index, test_index)\n",
    "        AUC_dict['xg'].append(xgauc)\n",
    "        \n",
    "        venv = VecEnv(env_list, train_index, test_index, num_process=num_process, state_dim=state_dim)\n",
    "        \n",
    "        model = build_model(embed_dim, state_dim, action_dim, feature_dim, action_max, \\\n",
    "                            y_max, model_family='gpt2').to(device)\n",
    "        agent = init_SACD_agent(state_dim, action_dim, action_max, device, embed_dim=embed_dim, \\\n",
    "                                weight_decay=5e-3, lr_actor=2e-5, lr_critic=5e-5, entropy_regularizer=0.03)\n",
    "        feature_optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 1e-5, 'weight_decay':5e-3}])\n",
    "        predict_buffer = ReplayBuffer()\n",
    "\n",
    "        # init predictor part\n",
    "        for freq in [1,5,20,2,50]:\n",
    "            train(venv, model, agent, feature_optimizer, predict_buffer, freq=freq, max_iter=10, burn_in=9)\n",
    "        \n",
    "        for freq in [1,3,15]:            \n",
    "            test(venv, model, agent, 'full', freq=freq)\n",
    "\n",
    "        venv.set_probe_penalty(0.1)\n",
    "        train(venv, model, agent, feature_optimizer, predict_buffer, freq=15, max_iter=100)        \n",
    "        # Test full observation-prediction\n",
    "        fcauc = test(venv, model, agent, run_id=run_id, kfold_id=i, penalty=-1, verbose=True)\n",
    "        fcauc = test(venv, model, agent, 'full', run_id=run_id, kfold_id=i, penalty=-1, verbose=True)\n",
    "        AUC_dict[1].append(fcauc)\n",
    "\n",
    "#         for freq in [5, 10]:\n",
    "#             print('-----------------------------------------------')\n",
    "#             penalty = freq * 0.05\n",
    "#             venv.set_probe_penalty(penalty)\n",
    "#             train(venv, model, agent, feature_optimizer, predict_buffer, max_iter=50, burn_in=10, freq=freq)\n",
    "\n",
    "#             # fcauc = test(venv, model, agent, 'full', run_id=run_id, kfold_id=i, penalty=-1)\n",
    "#             fcauc = test(venv, model, agent, freq=freq, run_id=run_id, kfold_id=i, penalty=penalty, verbose=True)\n",
    "#             AUC_dict[freq].append(fcauc)\n",
    "            \n",
    "        print('=================================================')\n",
    "        \n",
    "    print(AUC_dict)\n",
    "    return AUC_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3154a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUC_dict = {}\n",
    "for j in range(6):\n",
    "    dic = k_fold_test()\n",
    "    if( j == 0 ):\n",
    "        AUC_dict = dic\n",
    "    else:\n",
    "        for key in AUC_dict.keys():\n",
    "            AUC_dict[key].extend(dic[key])\n",
    "            \n",
    "for key in AUC_dict.keys():\n",
    "    print('key: ', key, 'mean: ', np.mean(AUC_dict[key]), 'median: ', np.median(AUC_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ef66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d1e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9726b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
