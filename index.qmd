---
title: Adaptive EMA Sampling for Alcohol Lapse Risk Prediction - A Reinforcement Learning Approach
author:
  - name: Jeongyeol Kwon 
    corresponding: false
    affiliations:
      - Wisconsin Institute for Discovery, University of Wisconsin-Madison
  - name: Kendra Wyant 
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Abstract here.
date: last-modified
bibliography: references.bib
number-sections: false 
editor_options: 
  chunk_output_type: console
---

```{r}
#| message: false
#| echo: false

# load libraries and read in data for printing results

library(tidyverse)
library(here)

theme_set(theme_classic())

aucs <- read_csv(here("data/auc_3_x_10.csv"),
                 show_col_types = FALSE)
posteriors <- read_csv(here("data/posteriors.csv"),
                       show_col_types = FALSE)
pp_tibble <- read_csv(here("data/pp_perf_tibble.csv"),
                       show_col_types = FALSE) |> 
  mutate(model = factor(model, 
                        levels = c("xgboost", "nn_full", "nn_lv1", 
                                   "nn_lv2", "nn_lv3", "nn_lv4"),
                        labels = c("xgboost", "no penalty", "Lv1", 
                                   "Lv2", "Lv3", "Lv4"))) 
contrasts <- read_csv(here("data/contrast.csv"),
                      show_col_types = FALSE)
freq <- read_csv(here("data/freq.csv"),
                      show_col_types = FALSE) |> 
  mutate(model = factor(model, 
                        levels = c("full", "lv1", 
                                   "lv2", "lv3", "lv4"),
                        labels = c("no penalty", "Lv1", 
                                   "Lv2", "Lv3", "Lv4")))
lapses <- read_csv(here("data/lapses.csv"),
                      show_col_types = FALSE)
```





# Introduction

Personal sensing is the collection and analyses of data collected via smartphone and other digital devices in the context of one's day-to-day life [@mohrPersonalSensingUnderstanding2017]. 

Personal sensing data can be used as inputs into machine learning algorithms to predict important clinical psychological outcomes. Existing literature suggests behavior (substance use [@wyantMachineLearningModels2024; @wyantLaggedPredictionsNextinprep; @baeMobilePhoneSensors2018; @waltersUsingMachineLearning2021; @robertsUsingMachineLearning2022; @baeLeveragingMobilePhone2023; @soysterPooledPersonspecificMachine2022]), mood (depression [@jacobsonPassiveSensingPrediction2020; @kimDepressionPredictionUsing2019; @razaviDepressionScreeningUsing2020]; anxiety [@jacobsonDigitalBiomarkersAnxiety2022]), cognitive processes (craving [@epsteinPredictionStressDrug2020; @dumortierClassifyingSmokingUrges2016]; suicidal thoughts [@czyzPredictingShorttermSuicidal2023]), and even future mental health diagnoses (@horwitzIntensiveLongitudinalAssessment2024) can be predicted with high accuracy and temporal precision from personal sensing data. 

Many mental health disorders, like substance use disorders, depression, and anxiety, are chronic conditions. After initial treatment and symptom reduction, individuals must continue to monitor their symptoms indefinitely. 

Machine learning models can help individuals self-monitor changes in their risk for symptom-relapse and provide information about intervenable targets for lowering their risk (e.g., urge surfing for strong craving, behavioral activation for low motivation, and mindful breathing for acute and distressing physiological arousal).

The existing successful prediction models have almost exclusively relied on ecological momentary assessment (EMA) as the primary personal sensing data collection method. EMA consists of daily or more frequent brief self-report surveys pushed to an individual's smartphone (i.e., prompts).

EMA offers insight into feelings, thoughts and behaviors. As such it is not surprising that it has been demonstrated to be highly predictive of clinical psychological outcomes. 

EMA items are also often easily mapped on to support recommendations and targets for interventions. For example, the relapse prevention literature for substance use provides extant evidence-based recommendations for coping with strong cravings, stressful situations, and risky situations [@marlattRelapsePreventionSecond2007; @witkiewitzTherapistsGuideEvidenceBased2007].

EMA models that predict substance use with high temporal precision (i.e., alcohol use in the next 1-24 hours) have relied on densely sampled EMA (4-8x daily [@wyantMachineLearningModels2024; @wyantLaggedPredictionsNextinprep; @waltersUsingMachineLearning2021; @soysterPooledPersonspecificMachine2022]). While it appears frequent EMA sampling can be sustainable for discrete sampling periods (e.g. 1-3 months) [@wyantAcceptabilityPersonalSensing2023; @jonesComplianceEcologicalMomentary2019], it is less clear whether such sampling rates can be maintained indefinitely. Some evidence suggests individuals with substance use disorders can maintain 1x daily EMA for up to 1 year [@moshontzProspectivePredictionLapses2021]. Still, recovery is dynamic and often lifelong. 

@wyantMachineLearningModels2024 have developed a machine learning model using 4x daily EMA that can predict alcohol lapses occurring in the next 24 hours with high accuracy (.90 area under the ROC curve [auROC]). This model can be used to provide personalized support recommendations to address immediate risks for possible lapses. For example, the model can be updated each day to provide individuals information about changes in their risk and make supportive recommendations based on the top features contributing to their risk.

While this model suggests promising clinical utility, frequent prompts each day could become burdensome. Engagement with the sensing method could drop leading to sparse feature sets. Similarly, individuals could become bored with the repetitive nature of questions, increasing the likelihood of careless or automatic responding without reflection.

Recognizing these limitations, we seek to apply a precision medicine approach to EMA sampling where individuals are only prompted to complete an EMA when the information is needed to maintain accurate lapse predictions. 

Specifically we ran a stimulation study thatâ€¦



# Backgrounds
Reinforcement Learning / Offline Reinforcement Learning

Partially Observed Systems / Partial Monitoring




# Methods

## Data Analytic Strategy

Models were run on an NVIDIA GeForce RTX 3090. <!--lets talk about what this is and if its important to include. If we keep it maybe add a sentence about why it was important to the analyses.-->

<!--Should we include a sentence saying models were trained and evaluated using Python (Version #) and list any major packages used?-->


### Problem Setup

#### Notation

$x_t$: $t^{th}$ survey response from a participant in the offline survey

$o_t$: observation to an agent returned from a simulated environment at $t^{th}$ time-step

$a_t$: action indicating whether to reveal the survey at $t^{th}$ time-step

$r_t$: reward (penalty) given to the agent

$T_t$: actual data-of-time when $t^{th}$ survey is collected


#### Simulated Environment

Our goal is to design a survey plan that maximizes the expected cumulative rewards (and minimize penalties) averaged over all participants during the entire episodes. We formulate the survey design problem as reinforcement learning (RL). We consider each set of surveys from the same participant as a *simulated* environment instantiated out of $N$ environments; each environment is simulated based on the offline survey data collected in. Specifically, in each episode with a participant randomly chosen from $N$ environments, at each $t^{th}$ time-step, the environment either asks (1) whether or not to collect the survey, or (2) to predict the next 24-hour window lapse. 

In the former case (1), when we get the next survey $o_t$ from the participant, we choose a binary action at deciding whether to collect the survey or not; that is, we do not observe the survey $o_t = null$ without penalty if $a_t = 0$. If we chose to collect the survey $a_t=1$, then the environment returns $o_t$ that simulates a patient's response based on surveys collected between last survey collection point and the current point. Specifically, if $t_p < t$ is the last time we collected the survey, the simulated environment takes the actual offline surveys $(x_{t_p+1},...,x_t)$; for the first 5 items that ask the values at *the most* intense moment, we take a maximum values over them, and for the remaining items, we take the values from $x_t$. In this case, the agent also gets a penalty $r_t < 0$. 

In the latter case (2), if the environment asks to predict the next 24-hour window lapse event $y_t$. In this phase, an agent gets a penalty if the latest survey collection point is 15 days before from the current query point, then it gets a big penalty $r_t \ll 0$ for not keeping the record up to date. This is a safe-guard: we want to regularly check up at least every 15 days. Otherwise, an agent gets no new observations, i.e., $x_t = o_t = null$, regardless of the chosen action. Instead, there is an unobserved penalty depending on the prediction accuracy. We calculate the prediction penalty in the two following ways:

a. $-c_0 * D_H(\mathbb{P}(y|x_1,...,x_t), \mathbb{P}(y|o_1,...,o_t))$

b. $c_0 * (1-y) * \min(p_0 - \mathbb{P}(y=1|o_1,...,o_t), 0) + c_1 * y * \min(0, \mathbb{P}(y=1|o_1,...,o_t) - p_1)$

where $D_H$ is Helligner distance, and $c_0, c_1, p_0, p_1 > 0$ are adjustable hyper-parameters. While we do not have an access to the true conditional probability, we use the estimated probability from the trained prediction network (see Model Architectures). 


### Problem Formulation
We consider the framework in the context of reinforcement learning in Markov Decision Processes. 

Reward uses estimated probabilities from feature network...





#### Feature Engineering

We use the similar sliding-window approach to (@wyantMachineLearningModels2024) for feature engineering, but with the simulated surveys $(o_1,...,o_t)$ instead of actual EMA data $(x_1,...,x_t)$. Specifically, we concatenate the following features:

- For the first 7 survey items, let $t_p$ be the starting of the sliding window for $k$-hours, i.e., $T_{t_p-1}<T_t - k<=T_{t_p}$. Compute mean, standard deviation and maximum of each collected survey item and concatenate them all. We also append the feature with number of collected surveys within the sliding-window $(o_{t_p}, ..., o_t)$. We use 5 window sizes $k=12,24,48,72,168$. 

- For the last 3 survey items that collected only at the morning, let $t_p$ be the starting of the sliding window for $k$-hours, i.e., $T_{t_p-1}<T_t - k<=T_{t_p}$. Compute mean, minimum and maximum of each collected survey item and concatenate them all. We also append the feature with number of collected surveys within the sliding-window $(o_{t_p}, ..., o_t)$. We use 3 window sizes $k=48,72,148$. 

- We take the mean and standard deviations of all collected surveys from the beginning.

- Finally, we append the feature with the actual time passed from the last survey. 

<!--finish updating math syntax - see https://qmd4sci.njtierney.com/math.html-->




### Model Architectures
Specifically, our training pipeline can be described as consisting of two networks (policy network and prediction network; @fig-method).

```{r}
#| label: fig-method
#| fig-cap: "Architecture for two-phase training."
#| fig-width: 6
#| fig-height: 3
#| echo: false 

knitr::include_graphics(path = here::here("figures/methods.png"), error = FALSE)
```

<!--I'm not sure what a summarizer network is. Maybe explain it in a few sentences.Our default configuration employs transformer architectures for the summarizing model. For implementing the transformer model, we adhere to the minimal implementation of NanoGPTâ€™s standard framework1. We use an Embedding layer for discrete actions. For the policy network, we employ the soft actor-critic method for discrete actions (SACD). We use the base two-layer fully-connected architecture for both actor and critic networks. The lapse prediction network uses a single Gated Recurrent Unit (GRU) network. <!--Move this to prediction network section?-->

<!--Jeongyeol: This network can be replaced by the manual feature-engineering we do-->

#### 1. Latent Representation

The engineered features will be passed through a building block to yield common representations for both prediction and reinforcement learning tasks.  


#### 2. Policy Network

In this phase, the goal is to learn a survey policy that maximizes the cumulative returns â€“ the sum of all rewards and penalties â€“ averaged over all participants. The policy takes all previous observations, which we call a historical context or simply history, and decides an action to take. <!--Jeongyeol: Then we do the feature engineering-->

New trajectories are with the current policy $\pi$, which takes an engineered feature Ï†(ht) as an input and outputs the next action. $\pi$ is updated to maximize the long-term returns of the system. <!--update math notation-->


<!--Add Partially Observed Markov Decision Processes and Objective Formulation and Algorithm 1 here?-->

#### 3. Prediction Network

Predictions were made into 24-hour windows given the engineered features (i.e., the probability of a lapse in the next 24 hours). The first prediction window for each participant started 24 hours from midnight on their study start date. Subsequent prediction window start times for each participant repeatedly rolled forward hour-by-hour until the end of their study participation.


### Training and Validation
The start and end date/time of past drinking episodes were reported on the first EMA item. A prediction window was labeled lapse if the start date/hour of any drinking episode fell within that window. A window was labeled no lapse if no alcohol use occurred within that window +/- 24 hours. If no alcohol use occurred within the window but did occur within 24 hours of the start or end of the window, the window was excluded. We ended up with a total of 274,179 labels ==> removed redundant label queries, ending up in 37216 label query points. <!--a bit of clarification needed: I examined all queries and observed that data seems redundant in consecutive label queries (without received surveys in-between); I know there are minor feature changes at different query times, but it seems that differences are minor and XGBoost performance does not seem to benefit from this duplication. To make my experiments a bit lighter, I compressed every consecutive label queries into one label query, which resulted in ~36000 labels in total.-->

We used 3 repeats of 10-fold cross-validation to evaluate our models with auROC. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). We used grouped cross-validation to assign all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participantâ€™s data from their own data. 
Prediction network takes an engineered representation of histories and predict the lapse event within the 24 hours window. JY TODO: Also need to mention Soft Actor-Critic RL training.  






# Experiments 

# Experimental Setup

XGBoost vs NN (2 Layer-FCN)
AUC is comparable with full observations.

Another baseline: Random dropout (80%~90%)

RL with Learning: Penalty and reward candidates?


# Evaluation Metric

1. AUC

2. How well survey resources are distributed <!--we want to discuss this one-->

2.a. How much variance in predicted survey outcomes?
2.b. How much variance in lapse prediction?
2.c. How much it is different from full survey?



### Model Evaluation

#### Bayesian Model

We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) from the 30 held-out test sets for our five best models (no penalty, Lv1, Lv2, Lv3, and Lv4). We used the default weakly informative priors. We set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within repeat.  

From the Bayesian model we obtained the posterior distribution (transformed back from logit) and Bayeisan CIs for auROCs all five models. To evaluate our modelsâ€™ overall performance we report the median posterior probability for auROC and Bayesian CIs. This represents our best estimate for the magnitude of the auROC parameter for each model. 

We then conducted Bayesian model comparisons to determine the probability that the penalized modelsâ€™ performances differed systematically from the baseline (no penalty) model. We also report the precise posterior probability for the difference in auROCs and the 95% Bayesian CIs for these comparisons.

## Results

### Participants
*Kendra will characterize sample of participants here*

### EMA Adherence and Sampling Frequency
Participants on average completed 3.1 (SD=0.6) of the 4 EMAs each day (78.4% adherence overall). Across the twelve weeks on study, EMA adherence ranged from 75.3% to 86.8%. 

The baseline (no penalty) model used all available EMA observations for all participants (proportion of EMAs used = 1.0). As penalties for revealing an EMA observation became higher, overall sampling frequency for EMA observations decreased. The median proportion of EMAs used in the penalized models were .41 (range = .24 - .56), .24 (range = .16 - .34), .12 (range .03 - .20), and .04 (range = 0 - .10) for Lv1, Lv2, Lv3, and Lv4 respectively. 

Figure @fig-ema presents histograms that display the distribution of the proportion of EMA observations revealed for each participant. 

<!-- note these plots depict 3 proportions for each subid - 1 for each repeat-->
```{r}
#| label: fig-ema
#| echo: false

freq |> 
  filter(model != "no penalty") |> 
  ggplot(aes(x = prop)) +
  geom_histogram(color = "black", fill = "light grey", bins = 40) +
  facet_wrap(~model) +
  geom_vline(aes(xintercept = med), freq |> 
               filter(model != "no penalty") |> 
               group_by(model) |> 
               mutate(med = median(prop))) +
  labs(title = "Histogram of proportion of surveys used by subid for each penalty",
       x = "proportion of EMA observations revealed")
```

There doesn't appear to be a relationship between lapse and proportion of EMA surveys revealed.
```{r}
#| echo: false

freq |>
  filter(model != "no penalty") |> 
  left_join(lapses, by = "subid") |> 
  ggplot(aes(x = prop, y = prop_lapse_days), alpha = .8) +
  geom_point() +
  facet_wrap(~model) +
  labs(title = "Relationship between lapses and survey frequency",
       x = "proportion of EMA surveys revealed",
       y = "proportion of study days with lapses")
```




### Model Evaluation

Raw median auROCs for xgboost and five neural net models are presented below.
```{r}
#| echo: false

aucs |> 
  group_by(n_repeat) |> 
  summarise(across(xgboost:nn_lv4, ~median(.x))) |> 
  ungroup() |> 
  summarise(across(xgboost:nn_lv4, ~median(.x))) |> 
  glimpse() 
```


Median posterior probabilities and 95% Bayesian credible intervals for auROC for the XGBoost and 5 neural network models are presented in the table below.
```{r}
#| echo: false

pp_tibble |> 
  knitr::kable(digits = 3)
```

Potential main figure
```{r}
#| echo: false

pp_tibble |> 
  filter(model != "xgboost") |> 
  mutate(penalty = c(0, .02, .05, .08, .12)) |> 
  ggplot(aes(x = penalty, y = pp_median)) +
  geom_point(aes(color = model), size = 2) +
  geom_segment(mapping = aes(x = penalty, y = pp_lower, yend = pp_upper,
                             color = model), linewidth = .75) +
  scale_y_continuous("area under ROC curve", limits = c(.5, 1)) +
  scale_x_continuous("penalty for revealed EMA", 
                     breaks = c(0, .02, .04, .06, .08, .10, .12)) +
  scale_color_viridis_d(option = "D") +
  labs(color = NULL) +
  theme(legend.position = "bottom")
```


Histograms of posterior probabilities for auROC for the 5 neural network models are presented below.  
```{r}
#| echo: false

posteriors |> 
  mutate(model = factor(model, 
                        levels = c("nn_full", "nn_lv1", "nn_lv2", 
                                   "nn_lv3", "nn_lv4"),
                        labels = c("no penalty", "Lv1", "Lv2", 
                                   "Lv3", "Lv4"))) |>
  filter(model != "xgboost") |> 
  ggplot() + 
  geom_histogram(aes(x = posterior), fill = "light grey", color = "black", 
                 bins = 30) +
  geom_segment(mapping = aes(y = 3000, yend = 3600, x = pp_median, 
                             xend = pp_median),
               data = subset(pp_tibble, model != "xgboost")) +
  geom_segment(mapping = aes(y = 3400, yend = 3400, x = pp_lower, xend = pp_upper),
                data = subset(pp_tibble, model != "xgboost")) +
  facet_wrap(~model, ncol = 1) +
  ylab("Posterior Probability Frequency") +
  xlab("Area Under ROC Curve") +
  theme_classic() 
```


### Model Comparisons

*Kendra will add a description of the model comparisons below once I see how we are referring to the different models. Essentially these comparisons suggest that we can go down to using about 40% of surveys (LV1) without any difference in model performance. It appears that the model does perform differently when going down to 25% of surveys (LV2) but this is not likely a clinically meaningful drop in performance. LV3 performs worse than LV1, but again it is not likely a clinically meaningful drop.*

```{r}
#| echo: false

contrasts |> 
  knitr::kable(digits = 3)
```





## Appendix

### Implementation Details

#### RL Algorithm

#### Feature Engineering

#### Hyperparameters

#### ...
