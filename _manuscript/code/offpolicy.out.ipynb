{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Comparison"
      ],
      "id": "a8505cc9-c3ea-4c8b-a1a7-4aed32a1e454"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "596662bd"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "This source code is licensed under the CC BY-NC license found in the\n",
        "LICENSE.md file in the root directory of this source tree.\n",
        "\"\"\"\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from munch import Munch\n",
        "import yaml\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ],
      "id": "3a6edc5b"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('ema.csv', sep=',')\n",
        "df = df.sort_values(['subid', 'dttm_obs'])[['dttm_obs', 'subid', 'ema_1', 'ema_2', 'ema_3', 'ema_4', \\\n",
        "                                            'ema_5', 'ema_6', 'ema_7', 'ema_8', 'ema_9', 'ema_10']]\n",
        "\n",
        "# Handling NA: drop the entire row if 4th entry is NA \n",
        "# For morning-only survey, set dummy=1 if survey reported, otherwise dummy=0\n",
        "df = df.fillna(-6.0)\n",
        "df = df[df['ema_4'] > -1]\n",
        "df['ema_dummy'] = (df['ema_8'] > -1)\n",
        "df.loc[df['ema_8'] < 0, ['ema_8', 'ema_9', 'ema_10']] = 0\n",
        "\n",
        "# Encoding: 'Yes/No -> 0/1'\n",
        "df = df.replace('No',0)\n",
        "df = df.replace('Yes',1)\n",
        "\n",
        "# Date-time encoding -- in hour unit \n",
        "df['date'] = (pd.to_datetime(df['dttm_obs']).astype('int64') // (10**9)) / 3600\n",
        "df2 = df[['subid', 'ema_1', 'ema_2', 'ema_3', 'ema_4', \\\n",
        "            'ema_5', 'ema_6', 'ema_7', 'ema_8', 'ema_9', 'ema_10', 'ema_dummy', 'date']]\n",
        "\n",
        "\n",
        "time_df = pd.read_csv('labels_1day.csv', sep=',')\n",
        "time_df = time_df.sort_values(['subid', 'dttm_label']) \n",
        "time_df['date'] = (pd.to_datetime(time_df['dttm_label']).astype('int64') // (10**9)) / 3600\n",
        "time_df = time_df.replace('no',0)\n",
        "time_df = time_df.replace('yes',1)\n",
        "\n",
        "time_df = time_df[['subid', 'lapse', 'date']]"
      ],
      "id": "d3e3ee3a"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def input_data(org_data, time_label):\n",
        "    # 24hr window right-index\n",
        "    fj = 0    \n",
        "    \n",
        "    # offset starting date-time to 0 \n",
        "    offset_time = org_data[0,-1] + 0.0\n",
        "    org_data[:,-1] -= offset_time\n",
        "    time_label[:,-1] -= offset_time\n",
        "    feature = np.mean(org_data[:, 1:-1], axis=0)\n",
        "    \n",
        "    # print(feature.shape)\n",
        "    dataset = []\n",
        "    tq = ts = 0\n",
        "    while(tq < time_label.shape[0] and ts < org_data.shape[0] ):\n",
        "        if(ts == org_data.shape[0] or (tq < time_label.shape[0] and time_label[tq, -1] < org_data[ts, -1])):\n",
        "            y = 1 if( time_label[tq,1] > 0 ) else 0\n",
        "            t_feature = np.concatenate([feature, np.mean(org_data[ts:, 2:-1], axis=0)])\n",
        "            dataset.append({'type':'query', 'out': y, 'time': time_label[tq,-1], 'feature': t_feature})\n",
        "            while(tq < time_label.shape[0] and time_label[tq,-1] < org_data[ts,-1] ):\n",
        "                tq += 1\n",
        "        else:\n",
        "            dataset.append({'type':'survey', 'obs': org_data[ts, 1:]+0.0, 'time': org_data[ts,-1]})\n",
        "            ts += 1\n",
        "\n",
        "    return dataset\n"
      ],
      "id": "7c2e7417"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 5, 6, 7, 9, 10, 11, 15, 16, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 42, 43, 44, 47, 48, 51, 52, 53, 54, 56, 58, 59, 63, 64, 65, 66, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 88, 90, 92, 93, 94, 97, 98, 99, 100, 103, 109, 110, 116, 117, 118, 119, 121, 128, 130, 131, 134, 135, 136, 137, 138, 139, 143, 149, 150, 156, 158, 161, 162, 163, 166, 167, 169, 171, 172, 175, 178, 179, 180, 181, 183, 185, 187, 188, 189, 190, 191, 192, 193, 196, 197, 200, 201, 203, 205, 207, 208, 209, 211, 212, 213, 214, 215, 218, 221, 222, 223, 224, 225, 230, 231, 232, 234, 236, 238, 240, 241, 242, 243, 245, 248, 252, 255, 259, 262, 263, 264, 265, 268, 270]"
          ]
        }
      ],
      "source": [
        "raw_data = df2.values.astype(np.float32)\n",
        "time_raw_data = time_df.values.astype(np.float32)\n",
        "\n",
        "env_list, subid_list = [], []\n",
        "print_list = []\n",
        "for subid in range(270):\n",
        "    sub_data = raw_data[raw_data[:,0]==(subid+1)]\n",
        "    time_label = time_raw_data[time_raw_data[:,0]==(subid+1)]\n",
        "    if( len(sub_data) == 0 or len(time_label) == 0 ):\n",
        "        continue\n",
        "          \n",
        "    subid_list.append(subid)\n",
        "    print_list.append(subid+1)\n",
        "    env_list.append(input_data(sub_data, time_label))\n",
        "\n",
        "print(print_list)"
      ],
      "id": "f8442e0f"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def input_xgboost_data(org_data, time_label):\n",
        "    j = [0,0,0,0,0]\n",
        "    fj = 0\n",
        "    window = [12, 24, 48, 72, 168]\n",
        "    \n",
        "    dataset = []\n",
        "    tq = ts = 0\n",
        "\n",
        "    while( tq < time_label.shape[0] ):\n",
        "        time = time_label[tq,-1]\n",
        "\n",
        "        while( ts < org_data.shape[0] and org_data[ts,-1] < time ):\n",
        "            ts += 1\n",
        "        \n",
        "        for k in range(5):\n",
        "            while( j[k] < ts-1 and org_data[j[k], -1] < time - window[k] ):\n",
        "                j[k] = j[k] + 1\n",
        "                   \n",
        "        if( ts == 0 ):\n",
        "            tq += 1\n",
        "            continue\n",
        "            \n",
        "        X1 = np.concatenate(([np.mean(org_data[j[k]:ts, 1:], axis=0) for k in range(5)], \\\n",
        "                            [np.min(org_data[j[k]:ts, 1:], axis=0) for k in range(5)] , \\\n",
        "                            [np.max(org_data[j[k]:ts, 1:], axis=0) for k in range(5)]), axis=1)\n",
        "        X1 = np.concatenate(X1)\n",
        "\n",
        "        sub_mean = np.mean(org_data[:ts], axis=0)\n",
        "        X = np.concatenate((X1, sub_mean[1:]))\n",
        "        X = np.concatenate((X, org_data[ts-1, 1:]))\n",
        "\n",
        "        y = 1 if (time_label[tq,1] > 0 ) else 0\n",
        "        tq += 1\n",
        "        \n",
        "        dataset.append({'in':X, 'out':y})\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def xgboost_main(train_index, test_index):\n",
        "    trainset = []\n",
        "    for m in train_index:\n",
        "        subid = subid_list[m] + 1\n",
        "        sub_data = raw_data[raw_data[:,0] == subid] \n",
        "        time_label = time_raw_data[time_raw_data[:,0]==subid]                \n",
        "        trainset.extend(input_xgboost_data(sub_data, time_label))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=500000, shuffle=True, drop_last=False)\n",
        "\n",
        "    testset = []\n",
        "    for m in test_index:\n",
        "        subid = subid_list[m] + 1\n",
        "        sub_data = raw_data[raw_data[:,0] == subid]     \n",
        "        time_label = time_raw_data[time_raw_data[:,0]==subid]                \n",
        "        testset.extend(input_xgboost_data(sub_data, time_label))\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=60000, shuffle=False, drop_last=False)\n",
        "\n",
        "    data_iter = iter(train_loader)\n",
        "    train_data = next(data_iter)\n",
        "    X = train_data['in']\n",
        "    y = train_data['out']\n",
        "\n",
        "    test_iter = iter(test_loader)\n",
        "    test_data = next(test_iter)\n",
        "\n",
        "    # Create regression matrices\n",
        "    dtrain_reg = xgb.DMatrix(X, y)\n",
        "\n",
        "    X_test = test_data['in']\n",
        "    y_test = test_data['out']\n",
        "    dtest_reg = xgb.DMatrix(X_test, y_test)\n",
        "\n",
        "    params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\", \"device\": \"cuda\"}\n",
        "\n",
        "    print(X.shape, X_test.shape)\n",
        "    \n",
        "    n = 10\n",
        "    xgmodel = xgb.train(\n",
        "       params=params,\n",
        "       dtrain=dtrain_reg,\n",
        "       num_boost_round=n,\n",
        "    )\n",
        "\n",
        "    preds = xgmodel.predict(dtest_reg)\n",
        "    print(len(y_test[y_test>0]), len(preds[preds>0.1]), len(preds))\n",
        "    print('XgBoost: AUC = %.2f' % roc_auc_score(y_test, preds))  #1\n"
      ],
      "id": "d6df7ad1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environment Configuration"
      ],
      "id": "3c1def68-89fe-4203-8677-8bdc920c6a71"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from env import Env, VecEnv\n",
        "\n",
        "# parallel env runs\n",
        "num_process = 50\n",
        "# survey window size (last \"window_size\" survey received points)\n",
        "window_size = 30\n",
        "\n",
        "\n",
        "# action encoding: \n",
        "# 0 - no survey, predict No\n",
        "# 1 - yes survey, predict No\n",
        "# 2 - no survey, predict Yes\n",
        "# 3 - yes survey, predict Yes\n",
        "action_dim = 1\n",
        "action_max = 4\n",
        "\n",
        "# feature encoding: mean of survey items\n",
        "feature_dim = 21\n",
        "\n",
        "# y encoding\n",
        "# 0 - no lapse within 24hr\n",
        "# 1 - yes lapse within 24hr\n",
        "state_dim, embed_dim, y_max = 12, 256, 2"
      ],
      "id": "77e5b749"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning"
      ],
      "id": "5099bdcd-0bab-4b20-97eb-b0a6a07b9d29"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# get trajectory sample batch\n",
        "def get_sample(traj, model, device):\n",
        "    beliefs = model(traj['prev_state'], traj['prev_action'], output_embedding=True)\n",
        "    \n",
        "    action = traj['action'].reshape(-1, 1).to(device)  # [B, n]\n",
        "    reward = traj['reward'].reshape(-1, 1).to(device)  # [B, n]\n",
        "    next_state = model(traj['next_state'], traj['next_action'], output_embedding=True)    \n",
        "    \n",
        "    state = beliefs.reshape(-1, embed_dim).detach()\n",
        "    next_state = next_state.reshape(-1, embed_dim).detach()\n",
        "    done = traj['done'].to(device)\n",
        "\n",
        "    return state,action,reward,next_state,done\n",
        "            \n",
        "# train function (without extra features)\n",
        "def train(venv, model, agent, buffer, predict_buffer, max_iter = 50, burn_in = 0, feature_weight = 0.0):\n",
        "    avg_list = []\n",
        "    \n",
        "    feature_optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 5e-5, 'weight_decay': 1e-8}])\n",
        "    ce_criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "    mse_criterion = torch.nn.MSELoss(reduction='none')\n",
        "    env = venv.venv[0]\n",
        "    \n",
        "    for episode in range(max_iter):\n",
        "        obs, info = venv.reset(mode='train')\n",
        "\n",
        "        cumr = 0\n",
        "        ts = torch.zeros(num_process,window_size, state_dim+1).to(device) \n",
        "        ta = torch.zeros(num_process,window_size, action_dim).to(int).to(device) \n",
        "\n",
        "        bf = model(ts[:,:,:-1], ta, output_embedding=True).detach()\n",
        "\n",
        "        sample_traj = []\n",
        "        predict_samples = []\n",
        "        \n",
        "        # generate trajectories\n",
        "        for t in range(1000):\n",
        "            if( episode < burn_in ):\n",
        "                actions = torch.zeros(num_process, action_dim).to(int) + 1\n",
        "            else:\n",
        "                actions, _, _ = agent.select_action(bf)\n",
        "                actions = actions.detach().cpu()\n",
        "                \n",
        "            obs, rewards, dones, infos = venv.step(actions)\n",
        "            cumr += sum(rewards)\n",
        "\n",
        "            # add transition data to dataset\n",
        "            all_done = True\n",
        "            for i in range(num_process):\n",
        "                if( infos[i] == -1 ):\n",
        "                    continue\n",
        "                \n",
        "                all_done = False\n",
        "                pts, pta = ts[i][:,:-1] + 0.0, ta[i] + 0\n",
        "                                \n",
        "                if (infos[i] != 'survey'):\n",
        "                    ts[i, -1, 0] = 0\n",
        "                    predict_samples.append({'action': pta, 'state': pts, 'out': infos[i]['out'], \\\n",
        "                                            'feature': infos[i]['feature']})\n",
        "                else:\n",
        "                    ts[i] = torch.cat([ts[i][1:], obs[i].view(-1, state_dim+1).to(device)], dim=0)\n",
        "                    ta[i] = torch.cat([ta[i][1:], actions[i].view(-1, action_dim).to(device)], dim=0)\n",
        "                    \n",
        "#                     cur_time = obs[i][-1]\n",
        "#                     st = 0\n",
        "#                     while(st < window_size and ts[i,st,-1] < cur_time - 168):\n",
        "#                         ts[i][st,:] *= 0\n",
        "#                         ta[i][st,:] *= 0\n",
        "#                         st+=1\n",
        "                        \n",
        "                nts, nta = ts[i][:,:-1] + 0.0, ta[i] + 0\n",
        "                sample_traj.append({'prev_state': pts, 'prev_action': pta, \\\n",
        "                            'action': actions[i], 'reward': rewards[i], \\\n",
        "                            'next_action': nta, 'next_state': nts, \\\n",
        "                            'done': dones[i]})\n",
        "\n",
        "            if( all_done ): \n",
        "                break\n",
        "                \n",
        "            bf = model(ts[:,:,:-1].reshape(num_process,-1,state_dim), ta.reshape(num_process,-1, 1), output_embedding=True).detach()\n",
        "\n",
        "        avg_cost = cumr / num_process\n",
        "        avg_list.append(avg_cost)\n",
        "        \n",
        "        buffer.add_episodes(sample_traj)\n",
        "        predict_buffer.add_episodes(predict_samples)\n",
        "        \n",
        "        # 1. train prediction first\n",
        "        train_loaders = predict_buffer.get_loader()\n",
        "        data_iter = iter(train_loaders)\n",
        "\n",
        "        for j in range(0, 200):\n",
        "            try:\n",
        "                traj = next(data_iter)\n",
        "\n",
        "                beliefs = model(traj['state'], traj['action'], output_embedding=True)\n",
        "                out_ys, out_features = model.predict_forward(beliefs)\n",
        "                ys = traj['out'].to(device).reshape(-1)\n",
        "                features = traj['feature'].to(device).reshape(-1, feature_dim)\n",
        "\n",
        "                predict_loss = ce_criterion(out_ys, ys).mean()\n",
        "                predict_loss += feature_weight * mse_criterion(out_features, features).mean()\n",
        "\n",
        "                predict_loss.backward()\n",
        "                feature_optimizer.step()\n",
        "                feature_optimizer.zero_grad()\n",
        "\n",
        "            except StopIteration:\n",
        "                train_loaders = predict_buffer.get_loader()\n",
        "                data_iter = iter(train_loaders)\n",
        "                continue\n",
        "\n",
        "        # 2. then train q\n",
        "        if( episode < burn_in ):\n",
        "            continue\n",
        "\n",
        "        train_loaders = buffer.get_loader()\n",
        "        data_iter = iter(train_loaders)\n",
        "\n",
        "        for j in range(0, 500):\n",
        "            try:\n",
        "                traj = next(data_iter)\n",
        "                batch = get_sample(traj, model, device)\n",
        "\n",
        "            except StopIteration:\n",
        "                train_loaders = buffer.get_loader()\n",
        "                data_iter = iter(train_loaders)\n",
        "                continue\n",
        "\n",
        "            agent.update(batch)\n",
        "\n",
        "        agent.update_target()\n",
        "\n"
      ],
      "id": "559e8580"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def test(venv, model, agent, mode='policy', run_id=0, kfold_id=0, penalty=-1.):\n",
        "    scatter = []\n",
        "    num_process = len(venv.test_ids)\n",
        "    \n",
        "    cnt, n_measure, n_hit = [0 for i in range(num_process)], [0 for i in range(num_process)], [0 for i in range(num_process)]\n",
        "    y, outy = [], []\n",
        "\n",
        "    env = venv.venv[0]\n",
        "    obs, info = venv.reset(mode='test')\n",
        "    cumr = 0\n",
        "    \n",
        "    ts = torch.zeros(num_process, window_size, state_dim+1).to(device)\n",
        "    ta = torch.zeros(num_process, window_size, action_dim).to(int).to(device)\n",
        "\n",
        "    bf = model(ts[:,:,:-1], ta, output_embedding=True)\n",
        "    bf = bf.detach()\n",
        "\n",
        "    for t in range(1000):\n",
        "        if( mode == 'policy' ):\n",
        "            actions, _, _ = agent.select_action(bf)\n",
        "            actions = actions.detach().cpu()\n",
        "        else:\n",
        "            actions = torch.zeros(num_process, action_dim).to(int) + 1\n",
        "\n",
        "        obs, rewards, dones, infos = venv.step(actions)\n",
        "        cumr += sum(rewards)\n",
        "        \n",
        "        predict_ys = model.predict_forward(bf)[0].detach()       \n",
        "   \n",
        "        # add transition data to dataset\n",
        "        all_done = True\n",
        "        for i in range(num_process):\n",
        "            if( infos[i] == -1 ):\n",
        "                continue\n",
        "\n",
        "            all_done = False\n",
        "            if (infos[i] != 'survey'):\n",
        "                check_y = nn.Softmax(dim=-1)(predict_ys[i]).reshape(y_max).detach().cpu()                \n",
        "                y.append(infos[i]['out'])\n",
        "                outy.append(check_y[1])\n",
        "            \n",
        "                ts[i][-1, 0] = 0\n",
        "            else:    \n",
        "                ts[i] = torch.cat([ts[i][1:], obs[i].view(-1, state_dim+1).to(device)], dim=0)\n",
        "                ta[i] = torch.cat([ta[i][1:], actions[i].view(-1, action_dim).to(device)], dim=0)\n",
        "\n",
        "#                 cur_time = obs[i][-1]\n",
        "#                 st = 0\n",
        "#                 while(st < window_size and ts[i,st,-1] < cur_time - 168):\n",
        "#                     ts[i][st,:] *= 0\n",
        "#                     ta[i][st,:] *= 0\n",
        "#                     st+=1\n",
        "\n",
        "                cnt[i] += 1\n",
        "                if( actions[i] % 2 == 1 ):\n",
        "                    n_measure[i] += 1\n",
        "                if( obs[i,1] > 0 ):\n",
        "                    n_hit[i] += 1\n",
        "\n",
        "        if( all_done ): \n",
        "            break\n",
        "                \n",
        "        bf = model(ts[:,:,:-1].reshape(num_process,-1,state_dim), ta.reshape(num_process,-1, action_dim), output_embedding=True).detach()\n",
        "\n",
        "    for i in range(num_process):\n",
        "        scatter.append([n_measure[i]/cnt[i], n_hit[i]/cnt[i]])\n",
        "        print(venv.test_ids[i], ':', n_measure[i], n_hit[i], cnt[i])\n",
        "    \n",
        "    path = os.path.join(\"./results\", f\"run_{run_id}_kfold_{kfold_id}_penalty_{penalty}.pt\")\n",
        "    things_to_save = {\n",
        "        'test_ids': venv.test_ids,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'agent_state_dict': agent.save_dict()\n",
        "    }\n",
        "    torch.save(things_to_save, path)\n",
        "    \n",
        "    \n",
        "    scatter = np.array(scatter)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(4, 3))  \n",
        "\n",
        "    plt.scatter(scatter[:,1], scatter[:,0])\n",
        "    plt.xlabel(\"Average Lapse Count\")\n",
        "    plt.ylabel(\"Survey Ratio\")\n",
        "    plt.show()\n",
        "\n",
        "    print('Average Survey Ratio:', np.mean(scatter[:,0]))\n",
        "\n",
        "    import xgboost as xgb\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    print('AUC = %.2f' % roc_auc_score(y, outy))  #1\n",
        "\n",
        "    \n",
        "    "
      ],
      "id": "10d51915"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Reinforcement Learning Result"
      ],
      "id": "7df3e262-2596-466e-9bca-42ee82d764db"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from SACD import init_SACD_agent\n",
        "from model import build_model\n",
        "from buffer import ReplayBuffer\n",
        "  \n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "\n",
        "def k_fold_test(k=5, run_id=0):\n",
        "    shuffled_index = list(range(151))\n",
        "    random.shuffle(shuffled_index)\n",
        "\n",
        "    for i in range(k):\n",
        "        st,et = i*151//k, (i+1)*151//k\n",
        "\n",
        "        test_index = shuffled_index[st:et]\n",
        "        train_index = list(set(range(151)) - set(test_index))\n",
        "        xgboost_main(train_index, test_index)\n",
        "        venv = VecEnv(env_list, train_index, test_index)\n",
        "        env = venv.venv[0]\n",
        "        \n",
        "        buffer = ReplayBuffer()\n",
        "        predict_buffer = ReplayBuffer()\n",
        "        model = build_model(embed_dim, state_dim, action_dim, feature_dim, action_max, y_max, model_family='gpt2').to(device)\n",
        "        agent = init_SACD_agent(state_dim, action_dim, action_max, device, belief_dim=embed_dim, \\\n",
        "                                lr_actor=1e-4, lr_critic=2e-4, entropy_regularizer=0.03)\n",
        "\n",
        "        # init predictor part\n",
        "        venv.set_probe_penalty(0.00)\n",
        "        train(venv, model, agent, buffer, predict_buffer, 25, burn_in=20)\n",
        "        \n",
        "        # Test full observation-prediction\n",
        "        test(venv, model, agent, 'full', run_id=run_id, kfold_id=i, penalty=-1)\n",
        "\n",
        "        for penalty in [0.02, 0.05, 0.08, 0.12]:\n",
        "            venv.set_probe_penalty(penalty)\n",
        "            train(venv, model, agent, buffer, predict_buffer, 10)\n",
        "            test(venv, model, agent, run_id=run_id, kfold_id=i, penalty=penalty)\n",
        "\n",
        "        print('-----------------------------------------------')\n",
        "        print('what about feature-weighted training?')\n",
        "\n",
        "        model = build_model(embed_dim, state_dim, action_dim, feature_dim, action_max, y_max, model_family='gpt2').to(device)\n",
        "        agent = init_SACD_agent(state_dim, action_dim, action_max, device, belief_dim=embed_dim, \\\n",
        "                                lr_actor=1e-4, lr_critic=2e-4, entropy_regularizer=0.03)\n",
        "\n",
        "        # init predictor part\n",
        "        venv.set_probe_penalty(0.00)\n",
        "        train(venv, model, agent, buffer, predict_buffer, 25, burn_in=20, feature_weight=0.2)\n",
        "        \n",
        "        # Test full observation-prediction\n",
        "        test(venv, model, agent, 'full', run_id=run_id, kfold_id=i, penalty=-1)\n",
        "\n",
        "        for penalty in [0.02, 0.05, 0.08, 0.12]:\n",
        "            venv.set_probe_penalty(penalty)\n",
        "            train(venv, model, agent, buffer, predict_buffer, 10, feature_weight=0.2)\n",
        "            test(venv, model, agent, run_id=run_id, kfold_id=i, penalty=penalty)\n",
        "            \n",
        "        print('===============================================')\n",
        "        "
      ],
      "id": "546951cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([218210, 204]) torch.Size([55929, 204])\n",
            "4055 13164 55929\n",
            "XgBoost: AUC = 0.88\n",
            "number of parameters: 0.79M\n",
            "Device set to : cuda:0"
          ]
        }
      ],
      "source": [
        "k_fold_test(5, 1)"
      ],
      "id": "ea162934"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_fold_test(10, 1)"
      ],
      "id": "c3ad9db7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_fold_test(10, 2)"
      ],
      "id": "a6d614db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_fold_test(10, 3)"
      ],
      "id": "d95c3d66"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  }
}